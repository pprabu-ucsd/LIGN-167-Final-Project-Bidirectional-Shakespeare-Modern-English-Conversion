{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this notebook is to train and run a bidirectional Marian MT model to translate Early Modern English to Modern English and Modern English to Early Modern English.\n"
      ],
      "metadata": {
        "id": "Z-kL-bfgNpQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We imported all of the needed libraries and modules for all aspects of running this model. Some of these are needed for saving checkpoints, compute metrics, tokenizing our data, training a Marian MT model, and running the model."
      ],
      "metadata": {
        "id": "dXvWwLxKNwca"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcqH0h-1mKTX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "from transformers import MarianTokenizer, MarianMTModel, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We upload our data to the Colab session using a file called `data.tsv`. This tsv has Shakespearean Early Modern English in the first column and its paired Modern English translation in the second column.\n"
      ],
      "metadata": {
        "id": "njjK-gUFNz8E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXFa4NZTXYsM"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "#file is called data.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we load in the Early Modern English and Modern English dataset as a `DatasetDict` with two columns. We then split this dataset into training (90%) and validation (10%) sets for loss analysis later.\n"
      ],
      "metadata": {
        "id": "_Ylcfkv6N2EQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXCjoeRhYDbn"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"csv\",\n",
        "    data_files={\"full\": str(Path(\"./data.tsv\"))},\n",
        "    delimiter=\"\\t\",\n",
        "    column_names=[\"shakespeare\", \"modern\"]\n",
        ")\n",
        "\n",
        "split_dataset = dataset[\"full\"].train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "valid_dataset = split_dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then prepare our dataset for training by adding special tokens needed for bidirectional input, tokenizing, processing the text, and loading the base model.\n"
      ],
      "metadata": {
        "id": "q5bjDVA0N5AP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6PDKEilZaT3"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer and base model\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-fr\"  # base MarianMT model\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "special_tokens = {\"additional_special_tokens\": [\"<to_modern>\", \"<to_shakespeare>\"]}\n",
        "tokenizer.add_special_tokens(special_tokens)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Preprocess function (no padding here; will use dynamic padding)\n",
        "def preprocess(batch):\n",
        "    src = [str(s) for s in batch[\"modern\"]]\n",
        "    tgt = [str(s) for s in batch[\"shakespeare\"]]\n",
        "    return tokenizer(src, text_target=tgt, max_length=128, truncation=True)\n",
        "\n",
        "def preprocess_bidirectional(batch):\n",
        "    modern_list = [str(s) for s in batch[\"modern\"]]\n",
        "    shakes_list = [str(s) for s in batch[\"shakespeare\"]]\n",
        "\n",
        "    # Build bidirectional sources/targets\n",
        "    src_texts = []\n",
        "    tgt_texts = []\n",
        "\n",
        "    for modern, shakes in zip(modern_list, shakes_list):\n",
        "        # Modern → Shakespeare\n",
        "        src_texts.append(\"<to_shakespeare> \" + modern)\n",
        "        tgt_texts.append(shakes)\n",
        "\n",
        "        # Shakespeare → Modern\n",
        "        src_texts.append(\"<to_modern> \" + shakes)\n",
        "        tgt_texts.append(modern)\n",
        "\n",
        "    # Tokenize both directions\n",
        "    return tokenizer(\n",
        "        src_texts,\n",
        "        text_target=tgt_texts,\n",
        "        max_length=128,\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "tokenized_train = train_dataset.map(preprocess_bidirectional, batched=True, remove_columns=train_dataset.column_names)\n",
        "tokenized_valid = valid_dataset.map(preprocess_bidirectional, batched=True, remove_columns=valid_dataset.column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then put together the hyperparameters we need for training, including things like learning rate, bath size, epoch amount, logging, saving, and evaluation. This then gets a `TrainingArguments` object, of which will later be used by the `Trainer`. We follow this by setting up the `Trainer` with dynamic padding and the compute metrics we established in a cell above. We are then finally ready to actually go through the training of our bidirection Marian MT model.\n",
        "\n"
      ],
      "metadata": {
        "id": "exa3RLPPN9dH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSULJ9EMZfFz"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./mt_shakespeare\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=50,\n",
        "    logging_steps=500,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,                # Mixed precision for faster GPU training\n",
        "    eval_strategy=\"epoch\"   # Skip evaluation to speed up training\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_valid,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the model trained, we move on to generating output from this model that we can test in various ways. To do so, we begin with this function that takes an input, follows the token on which direction to translate, generates output, and decodes that output.\n"
      ],
      "metadata": {
        "id": "nyJz3Sfs6G95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text):\n",
        "  encoded = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "  out_tokens = model.generate(**encoded, max_length=128)\n",
        "  return tokenizer.decode(out_tokens[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "5cIlAR-Y0-92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import our testing data as `test.tsv`.\n"
      ],
      "metadata": {
        "id": "dN6YnAt1OgV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "#import file called test.tsv"
      ],
      "metadata": {
        "id": "LNJM-AwGz_X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we load in the Early Modern English and Modern English test set as a `DatasetDict` with two columns.\n"
      ],
      "metadata": {
        "id": "C9xvK-UTOjNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "\n",
        "testset = load_dataset(\n",
        "    \"csv\",\n",
        "    data_files={\"full\": str(Path(\"./test.tsv\"))},\n",
        "    delimiter=\"\\t\",\n",
        "    column_names=[\"shakespeare\", \"modern\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "drFHCGZf0CKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there are line numbers in the test data, we eliminate those so that they don't interfere with our accuracy evaluation.\n"
      ],
      "metadata": {
        "id": "sYfkM0HlOkF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_numbers(row):\n",
        "    row[\"shakespeare\"] = re.sub(r\"\\d+\", \"\", str(row[\"shakespeare\"]))\n",
        "    row[\"modern\"] = re.sub(r\"\\d+\", \"\", str(row[\"modern\"]))\n",
        "    return row\n",
        "\n",
        "testset[\"full\"] = testset[\"full\"].map(remove_numbers)\n"
      ],
      "metadata": {
        "id": "L4c3Tjpfv7lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model was also trained with a tokenizer having `max_length=128` and we only have it generate things with `max_length=128` in the `translate` function, so here we get rid of all test cases that are excessively long since our model isn't built to perform well on that.\n"
      ],
      "metadata": {
        "id": "9pKTfQSfOn1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def too_long(testset):\n",
        "    sh_words = len(str(testset[\"shakespeare\"]).split())\n",
        "    mod_words = len(str(testset[\"modern\"]).split())\n",
        "    return (sh_words <= 25) and (mod_words <= 25)\n",
        "\n",
        "testset[\"full\"] = testset[\"full\"].filter(too_long)\n"
      ],
      "metadata": {
        "id": "Aw6Ta22Wzx4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a file `modern_test.tsv` that contains two columns: one for the original Modern English transcriptions and one for the Early Modern English translated into Modern English by our bidirectional MarianMT model. We will use this for testing evaluation.\n"
      ],
      "metadata": {
        "id": "8TU1nN22OotA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open(\"modern_test.tsv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.writer(f, delimiter=\"\\t\")\n",
        "    writer.writerow([\"original modern\", \"translated from early modern\"])\n",
        "\n",
        "    for i in range(len(testset[\"full\"])):\n",
        "        original = testset[\"full\"][i][\"shakespeare\"]\n",
        "        prompt = \"<to_modern> \" + str(original)\n",
        "\n",
        "        translated = translate(prompt)\n",
        "        print(i)\n",
        "        print(original)\n",
        "        print(translated)\n",
        "\n",
        "        writer.writerow([str(testset[\"full\"][i][\"modern\"]), translated])\n"
      ],
      "metadata": {
        "id": "m5Vi0CEE2C7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a file `shakespeare_test.tsv` that contains two columns: one for the original Early Modern English transcriptions and one for the  Modern English translated into Early Modern English by our bidirectional MarianMT model. We will use this for testing evaluation.\n"
      ],
      "metadata": {
        "id": "XXWO4MiyOx1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"shakespeare_test.tsv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.writer(f, delimiter=\"\\t\")\n",
        "    writer.writerow([\"original early modern\", \"translated from modern\"])\n",
        "\n",
        "    for i in range(len(testset[\"full\"])):\n",
        "        original = testset[\"full\"][i][\"modern\"]\n",
        "        prompt = \"<to_shakespeare> \" + str(original)\n",
        "\n",
        "        translated = translate(prompt)\n",
        "\n",
        "        writer.writerow([str(testset[\"full\"][i][\"shakespeare\"]), translated])\n"
      ],
      "metadata": {
        "id": "GsQUyCm12DoZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
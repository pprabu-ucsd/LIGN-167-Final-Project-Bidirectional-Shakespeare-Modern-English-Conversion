{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "#### This notebook fine-tunes a T5-small sequence-to-sequence model to translate between Modern English and Shakespearean English using instruction-based prefixes. It loads and cleans a parallel TSV dataset, builds a bidirectional training set, and evaluates performance with ROUGE and BLEU while logging training progress and saving the best checkpoint for inference.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installs all required libraries for fine-tuning and evaluating a T5, this includes the core transformer, evaluation, SentencePiece, and Weights & Biases integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip -q install -U transformers datasets accelerate evaluate sacrebleu sentencepiece wandb rouge_score nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports core libraries for data handling, model training, and evalution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, inspect, random\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    T5TokenizerFast,\n",
    "    T5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "import wandb\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Make runs reproducible-ish\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell authenticates Weights & Biases logging using a secret stored in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "import wandb\n",
    "\n",
    "WANDB_API_KEY = userdata.get(\"WANDB_API_KEY\")\n",
    "\n",
    "if WANDB_API_KEY:\n",
    "    wandb.login(key=WANDB_API_KEY)\n",
    "    print(\"W&B login ok.\")\n",
    "else:\n",
    "    print(\"No WANDB_API_KEY found in Colab Secrets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploads the TSV dataset into the Colab runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "print(\"Uploaded:\", list(uploaded.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the TSV file into a dataset and makes sure the columns are set up correctly. We also do light normalization like removing stray numbers and fixing whitespace so the model isn’t learning formatting junk instead of language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tsv(path=\"shakes_only_data.tsv\"):\n",
    "    # Try no-header assumption first\n",
    "    try:\n",
    "        ds = load_dataset(\n",
    "            \"csv\",\n",
    "            data_files=path,\n",
    "            delimiter=\"\\t\",\n",
    "            column_names=[\"shakespeare\", \"modern\"],\n",
    "            split=\"train\"\n",
    "        )\n",
    "        ex = ds[0]\n",
    "        if ex.get(\"shakespeare\") and ex.get(\"modern\"):\n",
    "            return ds\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: headered TSV\n",
    "    ds = load_dataset(\n",
    "        \"csv\",\n",
    "        data_files=path,\n",
    "        delimiter=\"\\t\",\n",
    "        split=\"train\"\n",
    "    )\n",
    "    cols = ds.column_names\n",
    "    if \"shakespeare\" not in cols or \"modern\" not in cols:\n",
    "        if len(cols) >= 2:\n",
    "            ds = ds.rename_columns({cols[0]: \"shakespeare\", cols[1]: \"modern\"})\n",
    "    return ds\n",
    "\n",
    "number_re = re.compile(r\"\\b\\d+\\b\")\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = number_re.sub(\"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def clean_example(ex):\n",
    "    return {\n",
    "        \"shakespeare\": clean_text(ex.get(\"shakespeare\", \"\")),\n",
    "        \"modern\": clean_text(ex.get(\"modern\", \"\")),\n",
    "    }\n",
    "\n",
    "raw = load_tsv(\"data.tsv\").map(clean_example)\n",
    "\n",
    "print(\"Columns:\", raw.column_names)\n",
    "print(\"Examples:\", len(raw))\n",
    "print(\"Sample:\")\n",
    "print(raw[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns each Shakespeare–Modern pair into two training examples so the model learns both directions. We add instruction-style prefixes to tell T5 which way to translate, then split everything into train and validation sets for fair evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bidir(ds):\n",
    "    src_list, tgt_list = [], []\n",
    "    for ex in ds:\n",
    "        s = ex[\"shakespeare\"].strip()\n",
    "        m = ex[\"modern\"].strip()\n",
    "        if not s or not m:\n",
    "            continue\n",
    "        # Modern -> Shakespeare\n",
    "        src_list.append(f\"translate modern to shakespeare: {m}\")\n",
    "        tgt_list.append(s)\n",
    "        # Shakespeare -> Modern\n",
    "        src_list.append(f\"translate shakespeare to modern: {s}\")\n",
    "        tgt_list.append(m)\n",
    "\n",
    "    return Dataset.from_dict({\"src\": src_list, \"tgt\": tgt_list})\n",
    "\n",
    "bidir = make_bidir(raw)\n",
    "print(\"Bidir size:\", len(bidir))\n",
    "\n",
    "# 90/10 split\n",
    "dataset = bidir.train_test_split(test_size=0.1, seed=seed)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the pretrained t5-small model and its tokenizer. We define how to tokenize our instruction prompts and targets, apply truncation for efficiency,\n",
    "and set up dynamic padding so batches stay clean and fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-small\"\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "max_src_len = 128\n",
    "max_tgt_len = 128\n",
    "\n",
    "def preprocess(batch):\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"src\"],\n",
    "        max_length=max_src_len,\n",
    "        truncation=True,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        text_target=batch[\"tgt\"],\n",
    "        max_length=max_tgt_len,\n",
    "        truncation=True,\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized = dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "print(tokenized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets up ROUGE and BLEU so we can quantify how close the model outputs are to the references. We use these to track validation performance and compare results across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    bleu_result = bleu.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=[[l] for l in decoded_labels]\n",
    "    )\n",
    "    rouge_result = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "\n",
    "    pred_lens = [np.count_nonzero(p != tokenizer.pad_token_id) for p in preds]\n",
    "\n",
    "    return {\n",
    "        \"bleu\": round(bleu_result[\"score\"], 4),\n",
    "        \"rouge1\": round(rouge_result[\"rouge1\"], 4),\n",
    "        \"rougeL\": round(rouge_result[\"rougeL\"], 4),\n",
    "        \"gen_len\": round(float(np.mean(pred_lens)), 4),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines the training settings like learning rate, batch size, epoch count, and checkpoint rules. Connecting with wnb here to visualize and track progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_args():\n",
    "    base_kwargs = dict(\n",
    "        output_dir=\"t5_shakespeare_bidir\",\n",
    "        run_name=\"t5-small-shakespeare-bidir\",\n",
    "        learning_rate=3e-4,\n",
    "        warmup_ratio=0.05,\n",
    "        weight_decay=0.01,\n",
    "        num_train_epochs=50,\n",
    "        per_device_train_batch_size=16 if device == \"cuda\" else 4,\n",
    "        per_device_eval_batch_size=16 if device == \"cuda\" else 4,\n",
    "        gradient_accumulation_steps=1,\n",
    "        predict_with_generate=True,\n",
    "\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,   # frequent enough to see epoch progress\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"rougeL\",\n",
    "        greater_is_better=True,\n",
    "\n",
    "        fp16=(device == \"cuda\"),\n",
    "        report_to=\"wandb\",\n",
    "    )\n",
    "\n",
    "    sig = inspect.signature(Seq2SeqTrainingArguments.__init__)\n",
    "    if \"evaluation_strategy\" in sig.parameters:\n",
    "        base_kwargs[\"evaluation_strategy\"] = \"epoch\"\n",
    "    else:\n",
    "        base_kwargs[\"eval_strategy\"] = \"epoch\"\n",
    "\n",
    "    return Seq2SeqTrainingArguments(**base_kwargs)\n",
    "\n",
    "training_args = build_training_args()\n",
    "training_args\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Builds the Seq2SeqTrainer with our model, datasets, collator, and metrics. This is sort of like a control panel where all of the training pipeline comes together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(tokenized[\"train\"]))\n",
    "print(\"Val size:\", len(tokenized[\"test\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs fine-tuning for the planned number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The trainer logs epoch automatically in its logging dict.\n",
    "\n",
    "train_out = trainer.train()\n",
    "train_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs one final evaluation on the validation set after training finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves the best version of the fine-tuned T5 model and tokenizer for any future use/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"t5_shakespeare_bidir_best\")\n",
    "tokenizer.save_pretrained(\"t5_shakespeare_bidir_best\")\n",
    "\n",
    "print(\"Saved: t5_shakespeare_bidir_best\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reloads the saved model and runs a few sample translations. This is a quick check to see how the model did. Validated by us humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"t5_shakespeare_bidir_best\").to(device)\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"t5_shakespeare_bidir_best\")\n",
    "\n",
    "def gen(prompt, max_new_tokens=80):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_beams=6,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "tests_modern = [\n",
    "    \"I have a bad feeling about this.\",\n",
    "    \"We must act quickly before night falls.\",\n",
    "    \"The council will meet tomorrow.\",\n",
    "]\n",
    "\n",
    "for t in tests_modern:\n",
    "    print(\"Modern:\", t)\n",
    "    print(\"Shakes:\", gen(f\"translate modern to shakespeare: {t}\"))\n",
    "    print()\n",
    "\n",
    "tests_shakes = [\n",
    "    \"I prithee, speak plain.\",\n",
    "    \"Wherefore dost thou linger in the night?\",\n",
    "]\n",
    "\n",
    "for t in tests_shakes:\n",
    "    print(\"Shakes:\", t)\n",
    "    print(\"Modern:\", gen(f\"translate shakespeare to modern: {t}\"))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WER Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text):\n",
    "  encoded = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "  out_tokens = model.generate(**encoded, max_length=256)\n",
    "  return tokenizer.decode(out_tokens[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "#import file called test.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "\n",
    "testset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\"full\": str(Path(\"./test.tsv\"))},\n",
    "    delimiter=\"\\t\",\n",
    "    column_names=[\"shakespeare\", \"modern\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_numbers(row):\n",
    "    row[\"shakespeare\"] = re.sub(r\"\\d+\", \"\", str(row[\"shakespeare\"]))\n",
    "    row[\"modern\"] = re.sub(r\"\\d+\", \"\", str(row[\"modern\"]))\n",
    "    return row\n",
    "\n",
    "testset[\"full\"] = testset[\"full\"].map(remove_numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def too_long(testset):\n",
    "    sh_words = len(str(testset[\"shakespeare\"]).split())\n",
    "    mod_words = len(str(testset[\"modern\"]).split())\n",
    "    return (sh_words <= 25) and (mod_words <= 25)\n",
    "\n",
    "testset[\"full\"] = testset[\"full\"].filter(too_long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = \"<to_modern> \" + str(testset[\"full\"][1][\"shakespeare\"])\n",
    "print(\"shakes: \" + inp)\n",
    "print(\"modern: \" + str(testset[\"full\"][1][\"modern\"]))\n",
    "\n",
    "print(translate(inp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"modern_test.tsv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    writer.writerow([\"original modern\", \"translated from early modern\"])\n",
    "\n",
    "    for i in range(len(testset[\"full\"])):\n",
    "        original = testset[\"full\"][i][\"shakespeare\"]\n",
    "        prompt = \"<to_modern> \" + str(original)\n",
    "\n",
    "        translated = translate(prompt)\n",
    "        print(i)\n",
    "        print(original)\n",
    "        print(translated)\n",
    "\n",
    "        writer.writerow([str(testset[\"full\"][i][\"modern\"]), translated])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"shakespeare_test.tsv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    writer.writerow([\"original early modern\", \"translated from modern\"])\n",
    "\n",
    "    for i in range(len(testset[\"full\"])):\n",
    "        original = testset[\"full\"][i][\"modern\"]\n",
    "        prompt = \"<to_shakespeare> \" + str(original)\n",
    "\n",
    "        translated = translate(prompt)\n",
    "\n",
    "        writer.writerow([str(testset[\"full\"][i][\"shakespeare\"]), translated])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

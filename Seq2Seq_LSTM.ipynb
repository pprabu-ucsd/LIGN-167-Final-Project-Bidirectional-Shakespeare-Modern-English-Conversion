{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook implements a LSTM-based sequence-to-sequence with attention as a bidirectional Early Modern English."
      ],
      "metadata": {
        "id": "C8NB0ygSUZAd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We begin by importing various libraries and modules, as well as changing the device for speed."
      ],
      "metadata": {
        "id": "BdDEUvXkaEQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, gc, time\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device: \" +  device)"
      ],
      "metadata": {
        "id": "qWjWhoKmTxVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then import our parallel data called `data.tsv` where one column is the Shakespearean translation and the other column is the paired Modern English."
      ],
      "metadata": {
        "id": "R8KlXNRiaMs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# file called data.tsv"
      ],
      "metadata": {
        "id": "T37w3eO5U-DU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we get rid of the line numbers from our data."
      ],
      "metadata": {
        "id": "V-kokmrIVHkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_lines = Path(\"./data.tsv\").read_text().splitlines()\n",
        "\n",
        "cleaned = []\n",
        "for line in raw_lines:\n",
        "    line = re.sub(r\"\\d+\", \"\", line).strip()\n",
        "    cleaned.append(line)\n"
      ],
      "metadata": {
        "id": "CFIKUaBAU-ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then split our data into Modern and Early Modern components."
      ],
      "metadata": {
        "id": "k4odLPZha5TT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_lines = []\n",
        "modern_lines = []\n",
        "\n",
        "for line in cleaned:\n",
        "    parts = line.split(\"\\t\")\n",
        "    if parts[0] == '':\n",
        "      continue\n",
        "    else:\n",
        "      shakespeare_lines.append(parts[0].strip())\n",
        "      modern_lines.append(parts[1].strip())"
      ],
      "metadata": {
        "id": "_J7alhTEVIPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we train a SentencePeice model on the combined corpus using `model_type` of `bpe`, then load the trained model and wrap it in a tokenizer function. This `tokenize` function takes a string, then returns a list of subword units that SentencePiece outputs."
      ],
      "metadata": {
        "id": "3t0t3EdAa_1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "from pathlib import Path\n",
        "\n",
        "combined_text = \"\\n\".join(shakespeare_lines + modern_lines)   # one sentence per line\n",
        "Path(\"combined.txt\").write_text(combined_text, encoding=\"utf8\")\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input='combined.txt',\n",
        "    model_prefix='shakes_mod',\n",
        "    vocab_size=2000,\n",
        "    character_coverage=1.0,\n",
        "    model_type='bpe',\n",
        "    bos_id=1, eos_id=2, pad_id=0, unk_id=3,\n",
        "    user_defined_symbols=''\n",
        ")\n",
        "\n",
        "sp = spm.SentencePieceProcessor(model_file='shakes_mod.model')\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Returns a list of sub‑word tokens produced by SentencePiece.\n",
        "    The model already lower‑cases (if you set `--lower_case=true`\n",
        "    during training) – we keep the original case so that proper nouns\n",
        "    stay distinguishable. Feel free to call `text.lower()` before passing\n",
        "    it if you prefer a case‑insensitive model.\n",
        "    \"\"\"\n",
        "    return sp.encode_as_pieces(text)"
      ],
      "metadata": {
        "id": "mVhfIdWthmYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This `build_joint_vocab` function makes all of the sentences (Modern and Shakespearean) into one vocabulary."
      ],
      "metadata": {
        "id": "76gFKxyJbZFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_joint_vocab(texts, min_freq=1):\n",
        "    counter = Counter()\n",
        "    for txt in texts:\n",
        "        counter.update(tokenize(txt))\n",
        "\n",
        "    vocab = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
        "    for token, freq in counter.items():\n",
        "        if freq >= min_freq:\n",
        "            vocab[token] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "joint_vocab = build_joint_vocab(shakespeare_lines + modern_lines, min_freq=1)\n",
        "vocab_size = len(joint_vocab)"
      ],
      "metadata": {
        "id": "5KPorTPdViFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This `to_ids` function converts sentences into a list of ids from the joint vocabulary, then adds `<SOS>` and `<EOS>` token ids as needed around the sentence. We then build lists of source ids and target ids, where they both contain the Modern and Shakespeare data, but in a parallel and opposite order."
      ],
      "metadata": {
        "id": "eBvP6Hxpbths"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_ids(text):\n",
        "    tokens = tokenize(text)\n",
        "    ids = [joint_vocab.get(tok, joint_vocab['<UNK>']) for tok in tokens]\n",
        "    return [joint_vocab['<SOS>']] + ids + [joint_vocab['<EOS>']]\n",
        "\n",
        "src_ids = [to_ids(s) for s in shakespeare_lines + modern_lines]\n",
        "tgt_ids = [to_ids(m) for m in modern_lines + shakespeare_lines]"
      ],
      "metadata": {
        "id": "yX95ewWxVlcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we generate our training (90%) and validation (10%) split."
      ],
      "metadata": {
        "id": "i_OjdMFFcERW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total = len(src_ids)\n",
        "train_sz = int(0.9 * total)\n",
        "\n",
        "train_src = src_ids[:train_sz]\n",
        "train_tgt = tgt_ids[:train_sz]\n",
        "\n",
        "val_src = src_ids[train_sz:]\n",
        "val_tgt = tgt_ids[train_sz:]"
      ],
      "metadata": {
        "id": "yOw2swwLVqwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `TranslationDataset` class below wraps the data."
      ],
      "metadata": {
        "id": "Vulxb_PseVpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_seqs, tgt_seqs):\n",
        "        self.src = src_seqs\n",
        "        self.tgt = tgt_seqs\n",
        "    def __len__(self): return len(self.src)\n",
        "    def __getitem__(self, idx):\n",
        "        return (torch.tensor(self.src[idx], dtype=torch.long),\n",
        "                torch.tensor(self.tgt[idx], dtype=torch.long))\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "    src_padded = pad_sequence(src_batch, batch_first=True,\n",
        "                              padding_value=joint_vocab['<PAD>'])\n",
        "    tgt_padded = pad_sequence(tgt_batch, batch_first=True,\n",
        "                              padding_value=joint_vocab['<PAD>'])\n",
        "    return src_padded, tgt_padded\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(TranslationDataset(train_src, train_tgt),\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          shuffle=True,\n",
        "                          collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(TranslationDataset(val_src, val_tgt),\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          shuffle=False,\n",
        "                          collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "3G-knwDjVs4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell builds our model.\n",
        "\n",
        "* The `Encoder` class defines an embedding layer with dimension 256, a 2-layer bidirectional LSTM with hidden size 128, and forward logic that returns the entire sequence of encoder states with final hidden and cell states.\n",
        "* The `Attention` class computes Bahdanau/additive attention using learned linear layers that transform the concatenation of encoder and decoder states, with projections to scalar alignment scores and a softmax normalization.\n",
        "* The `Decoder` class embeds the previous token, computes an attention context vector over encoder outputs, concatenates the embedding with the context vector, and passes the end result through a 2-layer unidirectional LSTM. A linear layer then maps the concatenation of the LSTM output, context vector, and embedding to vocabulary logits.\n",
        "* The `Seq2Seq` class serves as a wrapper that coordinates the encoder and decoder, putting together the bidirectional encoder states before initializing the decoder. During training, it uses teacher forcing and calculates loss. The training loop incorporates gradient clipping and early stopping based on validation loss."
      ],
      "metadata": {
        "id": "TEhIxB-ieV_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_sz, embed_dim, hidden_dim,\n",
        "                 num_layers, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_sz, embed_dim,\n",
        "                                      padding_idx=joint_vocab['<PAD>'])\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim,\n",
        "                            num_layers,\n",
        "                            batch_first=True,\n",
        "                            dropout=dropout,\n",
        "                            bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: [batch, src_len]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        # outputs: [batch, src_len, hidden*2] (bidirectional)\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hidden_dim * 3, hidden_dim)\n",
        "        self.v    = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden: [batch, hidden] (decoder's current hidden state)\n",
        "        # encoder_outputs: [batch, src_len, hidden*2]\n",
        "        src_len = encoder_outputs.size(1)\n",
        "\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)   # [batch, src_len, hidden]\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        attention = self.v(energy).squeeze(2)               # [batch, src_len]\n",
        "        return F.softmax(attention, dim=1)                  # normalized\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_sz, embed_dim, enc_hidden_dim,\n",
        "                 num_layers, attention, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(vocab_sz, embed_dim,\n",
        "                                      padding_idx=joint_vocab['<PAD>'])\n",
        "        # LSTM input = embed + context (enc_hidden_dim*2 because encoder is bidir)\n",
        "        self.lstm = nn.LSTM(embed_dim + enc_hidden_dim * 2,\n",
        "                            enc_hidden_dim,\n",
        "                            num_layers,\n",
        "                            batch_first=True,\n",
        "                            dropout=dropout)\n",
        "        self.fc_out = nn.Linear(enc_hidden_dim + enc_hidden_dim * 2 + embed_dim,\n",
        "                                vocab_sz)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_tok, hidden, cell, encoder_outputs):\n",
        "        # input_tok: [batch]  (token ids)\n",
        "        input_tok = input_tok.unsqueeze(1)                     # [batch, 1]\n",
        "        embedded = self.dropout(self.embedding(input_tok))     # [batch, 1, embed]\n",
        "\n",
        "        # ---- attention -------------------------------------------------\n",
        "        a = self.attention(hidden[-1], encoder_outputs)       # [batch, src_len]\n",
        "        a = a.unsqueeze(1)                                    # [batch, 1, src_len]\n",
        "        weighted = torch.bmm(a, encoder_outputs)              # [batch, 1, hidden*2]\n",
        "\n",
        "        # ---- LSTM input ------------------------------------------------\n",
        "        lstm_input = torch.cat((embedded, weighted), dim=2)   # [batch, 1, embed+hidden*2]\n",
        "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
        "\n",
        "        # ---- final prediction -----------------------------------------\n",
        "        prediction = self.fc_out(torch.cat((output.squeeze(1),\n",
        "                                            weighted.squeeze(1),\n",
        "                                            embedded.squeeze(1)), dim=1))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
        "        batch_sz, tgt_len = src.size(0), tgt.size(1)\n",
        "        vocab_sz = self.decoder.fc_out.out_features\n",
        "\n",
        "        # tensor to store decoder outputs\n",
        "        outputs = torch.zeros(batch_sz, tgt_len, vocab_sz).to(self.device)\n",
        "\n",
        "        # ---------- encoder -------------------------------------------------\n",
        "        enc_outputs, hidden, cell = self.encoder(src)\n",
        "\n",
        "        # Collapse bidirectional hidden states:\n",
        "        # hidden / cell: [num_layers*2, batch, hidden]\n",
        "        hidden = hidden.view(self.encoder.lstm.num_layers, 2, batch_sz, -1)\n",
        "        cell   = cell.view(self.encoder.lstm.num_layers, 2, batch_sz, -1)\n",
        "\n",
        "        # Use the **forward** direction only (or average both – here we average)\n",
        "        hidden = (hidden[:, 0, :, :] + hidden[:, 1, :, :]) / 2\n",
        "        cell   = cell[:, 0, :, :].contiguous()   # keep forward direction\n",
        "\n",
        "        # first input to the decoder is <SOS>\n",
        "        input_tok = tgt[:, 0]\n",
        "\n",
        "        for t in range(1, tgt_len):\n",
        "            output, hidden, cell = self.decoder(input_tok, hidden, cell, enc_outputs)\n",
        "            outputs[:, t, :] = output\n",
        "\n",
        "            # teacher forcing?\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input_tok = tgt[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "yueyKiQLVwnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then define our hyperparameters. `PATIENCE` is used for early stopping since we don't expect all of the epochs to be effective due to now having very much data."
      ],
      "metadata": {
        "id": "hWUuFoEceXGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBED_DIM = 256\n",
        "HIDDEN_DIM = 128\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.3\n",
        "LR = 5e-4\n",
        "EPOCHS = 50\n",
        "MAX_LEN = 128\n",
        "PATIENCE = 3"
      ],
      "metadata": {
        "id": "rxu6MAX8V1rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As declared before, we now initialize our model, optimizer (using Adam) and loss (cross-entropy)."
      ],
      "metadata": {
        "id": "H059EAsaeXkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder    = Encoder(vocab_size, EMBED_DIM, HIDDEN_DIM,\n",
        "                    NUM_LAYERS, DROPOUT)\n",
        "attention  = Attention(HIDDEN_DIM)\n",
        "decoder    = Decoder(vocab_size, EMBED_DIM, HIDDEN_DIM,\n",
        "                    NUM_LAYERS, attention, DROPOUT)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=joint_vocab['<PAD>'])"
      ],
      "metadata": {
        "id": "_WbMv7g0V3Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `maybe_truncate` discards anything after `MAX_LEN` in a batch."
      ],
      "metadata": {
        "id": "YVEMeSNJeYFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def maybe_truncate(tensor):\n",
        "    if tensor.size(1) > MAX_LEN:\n",
        "        return tensor[:, :MAX_LEN]\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "eQ0NBGrkV41b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function `train_epoch` takes our sequence-to-sequence model, a `loader` that gives batches of pairs, an `optimizer` to update parameters, a loss function (`criterion`), and a limit on the gradient vector (`clip`). It loops over all batches (pairs of data) to train the model on this epoch's loss and update parameters."
      ],
      "metadata": {
        "id": "8d9UQsKclbSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, loader, optimizer, criterion, clip=1.0):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for src, tgt in loader:\n",
        "        src = maybe_truncate(src).to(device)\n",
        "        tgt = maybe_truncate(tgt).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt)                 # [batch, tgt_len, vocab]\n",
        "\n",
        "        # reshape for CrossEntropy (ignore first token <SOS>)\n",
        "        output = output[:, 1:, :].reshape(-1, vocab_size)\n",
        "        tgt    = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, tgt)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(loader)"
      ],
      "metadata": {
        "id": "ZOSNPbklV8-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function `evaluate` takes our sequence-to-sequence model, a `loader` that gives batches of pairs, and an `optimizer` to update parameters. It loops over all batches (pairs of data) to find the loss of the epoch on the validation set."
      ],
      "metadata": {
        "id": "TI91f2uMmJB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in loader:\n",
        "            src = maybe_truncate(src).to(device)\n",
        "            tgt = maybe_truncate(tgt).to(device)\n",
        "\n",
        "            output = model(src, tgt, teacher_forcing_ratio=0.0)\n",
        "\n",
        "            output = output[:, 1:, :].reshape(-1, vocab_size)\n",
        "            tgt    = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, tgt)\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(loader)"
      ],
      "metadata": {
        "id": "czp0BSwlV_8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then go through the training loop in the cell below to put this all together. We go through the declared number of epochs and update parameters while getting training and validation loss unless there has been no improvement for `PATIENCE` times."
      ],
      "metadata": {
        "id": "7_W-AG9UmJfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_val = float('inf')\n",
        "no_improve = 0\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    start = time.time()\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
        "    val_loss   = evaluate(model, val_loader, criterion)\n",
        "    elapsed = time.time() - start\n",
        "\n",
        "    mins, secs = divmod(int(elapsed), 60)\n",
        "\n",
        "    if val_loss < best_val:\n",
        "        best_val = val_loss\n",
        "        torch.save(model.state_dict(), 'best_joint_model.pt')\n",
        "        no_improve = 0\n",
        "        status = \"Saved best\"\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        status = f\"No improvement ({no_improve}/{PATIENCE})\"\n",
        "\n",
        "    print(f\"Epoch {epoch:02} | {mins}m {secs}s\")\n",
        "    print(f\"   Train loss: {train_loss:.4f} | PPL: {torch.exp(torch.tensor(train_loss)):.2f}\")\n",
        "    print(f\"   Val   loss: {val_loss:.4f} | PPL: {torch.exp(torch.tensor(val_loss)):.2f}  {status}\")\n",
        "\n",
        "    if no_improve >= PATIENCE:\n",
        "        print(f\"Early stopping after {epoch} epochs (val loss hasn't improved).\")\n",
        "        break"
      ],
      "metadata": {
        "id": "sJ-iT4riWCU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we do not stop early, we load the best checkpoint below to prepare for inference."
      ],
      "metadata": {
        "id": "bM5INLhqou8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('best_joint_model.pt', map_location=device))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "Z9-_6IwFWElC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then create a few functions to simplify inference. The `translate` function tokenizes an input sentence, encodes it, and autoregressively generates an output sequence without teacher forcing until either an `<EOS>` token is produced or the maximum length is reached."
      ],
      "metadata": {
        "id": "ltazt_qYo2y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence: str,\n",
        "              src_vocab: dict,\n",
        "              tgt_vocab: dict,\n",
        "              max_out_len: int = 50) -> str:\n",
        "    \"\"\"\n",
        "    `src_vocab` and `tgt_vocab` are BOTH the *joint* vocab.\n",
        "    The function only cares about which language you feed in as `sentence`.\n",
        "    \"\"\"\n",
        "    src_ids = to_ids(sentence)               # uses joint_vocab internally\n",
        "    src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_outputs, hidden, cell = model.encoder(src_tensor)\n",
        "\n",
        "        # collapse bidirectional hidden states (same logic as in Seq2Seq.forward)\n",
        "        hidden = hidden.view(model.encoder.lstm.num_layers, 2,\n",
        "                             1, -1)\n",
        "        cell   = cell.view(model.encoder.lstm.num_layers, 2,\n",
        "                           1, -1)\n",
        "\n",
        "        hidden = (hidden[:, 0, :, :] + hidden[:, 1, :, :]) / 2\n",
        "        cell   = cell[:, 0, :, :].contiguous()\n",
        "\n",
        "        # first decoder input = <SOS>\n",
        "        input_tok = torch.tensor([tgt_vocab['<SOS>']], device=device)\n",
        "\n",
        "        generated = []\n",
        "\n",
        "        for _ in range(max_out_len):\n",
        "            out, hidden, cell = model.decoder(input_tok, hidden, cell, enc_outputs)\n",
        "            pred_id = out.argmax(1).item()\n",
        "            if pred_id == tgt_vocab['<EOS>']:\n",
        "                break\n",
        "            generated.append(pred_id)\n",
        "            input_tok = torch.tensor([pred_id], device=device)\n",
        "\n",
        "    inv_vocab = {idx: tok for tok, idx in tgt_vocab.items()}\n",
        "    tokens = [inv_vocab.get(i, '<UNK>') for i in generated]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "# Helper wrappers for the two directions (they both reuse the joint vocab)\n",
        "def shakespeare_to_modern(sentence):\n",
        "    return translate(sentence, joint_vocab, joint_vocab)\n",
        "\n",
        "def modern_to_shakespeare(sentence):\n",
        "    return translate(sentence, joint_vocab, joint_vocab)"
      ],
      "metadata": {
        "id": "lFM8B0q9WFGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Output Setup\n",
        "(Code explained in the Marian MT notebook. Only difference is not using the `translate` function since we have `shakespeare_to_modern` and `modern_to_shakespeare` in this context.)"
      ],
      "metadata": {
        "id": "s30JCbXmCnP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "#import file called test.tsv"
      ],
      "metadata": {
        "id": "LNJM-AwGz_X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "\n",
        "testset = load_dataset(\n",
        "    \"csv\",\n",
        "    data_files={\"full\": str(Path(\"./test.tsv\"))},\n",
        "    delimiter=\"\\t\",\n",
        "    column_names=[\"shakespeare\", \"modern\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "drFHCGZf0CKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_numbers(row):\n",
        "    row[\"shakespeare\"] = re.sub(r\"\\d+\", \"\", str(row[\"shakespeare\"]))\n",
        "    row[\"modern\"] = re.sub(r\"\\d+\", \"\", str(row[\"modern\"]))\n",
        "    return row\n",
        "\n",
        "testset[\"full\"] = testset[\"full\"].map(remove_numbers)\n"
      ],
      "metadata": {
        "id": "L4c3Tjpfv7lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def too_long(testset):\n",
        "    sh_words = len(str(testset[\"shakespeare\"]).split())\n",
        "    mod_words = len(str(testset[\"modern\"]).split())\n",
        "    return (sh_words <= 25) and (mod_words <= 25)\n",
        "\n",
        "testset[\"full\"] = testset[\"full\"].filter(too_long)\n"
      ],
      "metadata": {
        "id": "Aw6Ta22Wzx4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open(\"modern_test.tsv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.writer(f, delimiter=\"\\t\")\n",
        "    writer.writerow([\"original modern\", \"translated from early modern\"])\n",
        "\n",
        "    for i in range(len(testset[\"full\"])):\n",
        "        original = str(testset[\"full\"][i][\"shakespeare\"])\n",
        "\n",
        "        translated = shakespeare_to_modern(original)\n",
        "        print(i)\n",
        "        print(original)\n",
        "        print(translated)\n",
        "\n",
        "        writer.writerow([str(testset[\"full\"][i][\"modern\"]), translated])\n"
      ],
      "metadata": {
        "id": "m5Vi0CEE2C7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"shakespeare_test.tsv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.writer(f, delimiter=\"\\t\")\n",
        "    writer.writerow([\"original early modern\", \"translated from modern\"])\n",
        "\n",
        "    for i in range(len(testset[\"full\"])):\n",
        "        original = str(testset[\"full\"][i][\"modern\"])\n",
        "\n",
        "        translated = modern_to_shakespeare(original)\n",
        "\n",
        "        writer.writerow([str(testset[\"full\"][i][\"shakespeare\"]), translated])\n"
      ],
      "metadata": {
        "id": "GsQUyCm12DoZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}